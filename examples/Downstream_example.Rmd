---
title: "Analysis pipeline"
output: html_notebook
---
    

We'll provide an example of current functionality of downstream analysis.

First let's load the datasets and peak into their structure.

```{r}
library(data.table)
library(igraph)
library(purrr)
library(glue)
library(scDiffCom)
library(arules)
library(pheatmap)
library(RColorBrewer)

source("../src/downstream_processing.R")


DATA_PATH = "../../single-cell-communication/data/pipeline_output/analysis_4_data_diffcom_filter.rds"


cols = get_default_colnames()
datasets = readRDS(DATA_PATH)

message("Dataset structure:")
str(datasets$tms_droplet)

message("Dataset sizes")
lapply(datasets, dim)

message("Dataset tissues")
lapply(datasets, function(x) {unique(x[["TISSUE"]])})
```

Let's stick to an analysis of TMS droplet data and let's first perform the filtering. There are a couple of variables here that need to be decided upon:
    - cutoff on LR_SCORE young and old: below it's computed as the min of 20-percentile on young and old.
    - reassignment of significance type: reassignment can be specified as a list type argument to analyze_CCI as exemplified below.

Sticking to a functional programming style, generally functions don't mutate the data tables they act upon. Therefore the step of removing undetected interactions has to be done separately and explicitely (as below).    

```{r}
data = datasets$tms_droplet

cutoff_score_young = quantile(data[LR_DETECTED_young == TRUE]$LR_SCORE_young, 0.20)
cutoff_score_old = quantile(data[LR_DETECTED_old == TRUE]$LR_SCORE_old, 0.20)
cutoff = min(cutoff_score_young, cutoff_score_old)

data = analyze_CCI(data, 
                   cutoff_score_old = cutoff,
                   cutoff_score_young = cutoff,
                   reassignment = list(
                       
                       # "TTFU" = "TTFU",
                       # "TTFD" = "TTFD",
                       # "TTTU" = "TTTU",
                       # "TTTD" = "TTTD",
                       
                       "TFTU" = "FFFU",  # TFTU --(assigned to)--> FFFU
                       # "TFTD" = "TFTD",
                       "TFFU" = "FFFU",
                       "TFFD" = "TFTD",
                       
                       # "FTTU" = "FTTU",
                       "FTTD" = "FFFD",
                       "FTFU" = "FTTU",
                       "FTFD" = "FFFD",
                       
                       "FFTU" = "FFFU",
                       "FFTD" = "FFFD",
                       # "FFFU" = "FFFU",
                       # "FFFD" = "FFFD,
                   ))

data = data[!(CASE_TYPE %in% c("FFFU", "FFFD"))]
dim(data)
```


Now let's perform overrepresentation ananlysis. In contrast to previous version, the output has been aggregated into a single data.table. By selecting a "Category", one can filter and obtain the ora for a particular concept of interest. Next I'd like to clarify some of the columns:

    - the counts columns shows the counts that have been used in the contingency table representation.
    - OR - odds ratio
    - pval and pval_adj - p-values from fisher's exact test.
    - Kulc_distance and imbalance ratio - clarified below.

It's best to show the computation on the contingency table:
                                    
                                    In sign | Not in sign |
Concept_of_interest (e.g. L_gene) |     a   |     b       |
Not concept of int.               |     c   |     d       |

OR = a * d / b * c

I simplified the formulas for the case of Kulc and imbalance_ratio applied to contingency tables:
Kulc_dist = avg(a/(a+b) , a/(a+c))
Imbalance_ratio = |b-c| / (a + ...)  # forgot the denominator and I'm on the bus, it's a form of normalization :)

In our case I consider the p-value from Fisher's exact test and the OR is the right metric as an overrepresentation one, because it takes into account d and it makes sense in our case to take into account d: if we held a,b,c constant and increase d, we'd like our overrepresentation metric to increase, since b/d is small and a/c is large <=> fraction of entries with the concept of interest in significant interactions increases in comparison to the fraction of entries with the concept of interest in not significant. 

Kulc distance and imbalance ratio are generally useful for market basket analysis, but currently I don't use them (maybe during the analysis an application might arise). 


```{r}
dt_ora = analyze_ORA(data, cols)
# pairs(ora[pval_adjusted < 0.05, .(OR, pval_adjusted, Kulc_distance, Imbalance_ratio)])

head(dt_ora)
```

Let's analyze the frequent itemsets. Currently it's being used in a limited fashion to look at patterns that involve a ligand celltype and receptor gene or ligand gene and receptor celltype. Those are stored separately as sub1 and sub2 entries in the resulting fpm list.

It is important to clarify some of the arguments:
    
    - support: a threshold on the frequency of a set. If the frequency of a set in the database is below support, that set is dropped.
    
    - confidence: an argument to restrict rules. Rules have the form (LHS) => (RHS), where LHS - left hand side and RHS - right. It models an implication logical relationship, and these are generally called association rules. The confidence of a rule is basically P(LHS and RHS | LHS)), i.e. it computes the frequency of LHS and RHS found together from the sets where LHS is identified.
    
    - target: specifies the kind of itemsets to be computed. This addresses the issues of having a particular kind of redundancy in the frequent itemsets, e.g. if (Marrow, Lymphocyte, Granulocyte, COL1A1, ARB) is frequent, so is any subset of this. In fact, any subset of this is at least as frequent as the initial set. To reduce the amount of such sets, closed frequent itemsets or maximal frequent itemsets can be computed. Closed frequent itemsets basically drops any subsets of a frequent itemset.

```{r}
fpm = analyze_FreqItemSets(data, cols, target = "closed frequent itemsets",
                           support = 0.00001, confidence = 0.01)

head(fpm$sub1)
head(fpm$sub2)
```

Let's do the heatmaps. First we convert dt_ora to dt_ctypes, a data.table that stores what can be thought of as edge data, where every row describes an edge between a ligand and receptor celltype with edge attributes: OR, OR_UP, OR_DOWN. This data.table is exploited to build the heatmaps.

```{r}

HEATMAPS_DIR = "../../single-cell-communication/scripts/downstream_processing_dev/notebooks/heatmaps_dir"  # specify with or without "/" as last char :)

dt_ctypes = get_celltypes_enrichment(dt_ora, cols)
head(dt_ctypes)

build_heatmaps(dt_ctypes, cols, HEATMAPS_DIR)
message("Double-check: ligands are rows? cols?")
message("Need to add the Ligand vs receptor logFC plots")
```
