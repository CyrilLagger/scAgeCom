---
title: "Analysis pipeline"
---

This notebook presents the filtered, ora and fpm results. Other notebooks contain the construction of heatmaps and graphs, as well as further exploration of the results.

*NOTE*: Filtering is performed now in the scDiffCom package, so we here we just load the output.


```{r}
library(data.table)
library(purrr)
library(arules)
library(glue)  # should eliminate this dependency

RESULTS_PATH = '../../data_scAgeCom/testing/scdiffcom_tms_facs_mixed_Spleen.rds'
results = readRDS(RESULTS_PATH)
str(results, max.level = 1)
```

    
*NOTE*: Overrepresentation analysis is performed now in the scDiffCom package.

Now to the overrepresentation analysis. I'd like to clarify some of the columns:

    - the counts columns shows the counts that have been used in the contingency table representation.
    - OR - odds ratio
    - pval and pval_adj - p-values from fisher's exact test.
    - Kulc_distance and imbalance ratio - clarified below.

It's best to show the computation on the contingency table:
                                    
                                    In sign | Not in sign |
Concept_of_interest (e.g. L_gene) |     a   |     b       |
Not concept of int.               |     c   |     d       |

OR = a * d / b * c

I simplified the formulas for the case of Kulc and imbalance_ratio applied to contingency tables:
Kulc_dist = avg(a/(a+b) , a/(a+c))
Imbalance_ratio = |b-c| / (a + ...)

In our case I consider the p-value from Fisher's exact test and the OR is the right metric as an overrepresentation one, because it takes into account d and it makes sense in our case to take into account d: if we held a,b,c constant and increase d, we'd like our overrepresentation metric to increase, since b/d is small and a/c is large <=> fraction of entries with the concept of interest in significant interactions increases in comparison to the fraction of entries with the concept of interest in not significant. 

Kulc distance and imbalance ratio are generally useful for market basket analysis, but currently we don't depend on them. 


```{r}
results$ORA
```

Let's analyze the frequent itemsets.

It is important to clarify some of the arguments of the frequent itemset analysis:
    
    - support: a threshold on the frequency of a set. If the frequency of a set in the database is below support, that set is dropped.
    
    - confidence: an argument to restrict rules. Rules have the form (LHS) => (RHS), where LHS - left hand side and RHS - right. It models a logical relationship of implication - "=>", more generally called association rules. The confidence of a rule is basically P(LHS and RHS | LHS)), i.e. it computes the frequency of LHS and RHS found together from the sets where LHS is found.
    
    - target: specifies the kind of itemsets to be computed. This addresses the issues of having a particular kind of redundancy in the frequent itemsets, e.g. if (Marrow, Lymphocyte, Granulocyte, COL1A1, ARB) is frequent, so is any subset of this. In fact, any subset of this is at least as frequent as the initial set. To reduce the amount of such sets, closed frequent itemsets or maximal frequent itemsets can be computed. Closed frequent itemsets basically drops any subsets of a frequent itemset.

```{r}

source("../src/fpm.R")

results$FPM = analyze_FreqItemSets(
    data = results$scdiffcom_dt_filtered,
    support = 0.001,
    confidence = 0.1
  )

results$FPM
```

*NOTE*: Heatmaps were done here, but moved to analysis_6b together with bipartite graphs.
